{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e99e55-341e-43da-8c1f-afee52bb0e3c",
   "metadata": {},
   "source": [
    "## Phase 2: Find Processing Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96fd6e95-d1fa-48f2-bd0c-3bef4f9577fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3721b79-5019-4f5a-916a-a709f08ae1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class -> Try if python utils connects to notebook: My_Test_Class \n",
      "\n",
      "Function -> Try if python utils connects to notebook: My_Test_Function\n"
     ]
    }
   ],
   "source": [
    "# connects to utils and run a test for connectivity\n",
    "from Utils.test_class_func import Test_py \n",
    "from Utils.test_class_func import test_py  \n",
    "print(Test_py(\"My_Test_Class\").print_(), \"\\n\")\n",
    "print(test_py(\"My_Test_Function\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbec7fd7-6fc7-4346-ae93-4d26542a2e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-21 21:42:12.084380: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-21 21:42:12.298282: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755826932.322211   69218 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755826932.333239   69218 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-21 21:42:12.383390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import functions \n",
    "from Utils.preporcessing_utils import data_loading \n",
    "from Utils.preporcessing_utils import labels_encoding\n",
    "from Utils.preporcessing_utils import split_data\n",
    "from Utils.preporcessing_utils import image_iterators\n",
    "from Utils.preporcessing_utils import ablation\n",
    "from Utils.models_utils import Basic_Custom_CNN\n",
    "from Utils.evaluation_utils import Evaluation\n",
    "from Utils.save_data_utils import Save_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc3f4e7-0e09-4cb6-a4dd-38af1616dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d801ef-a7c3-4803-bd18-3c8b0c1633e3",
   "metadata": {},
   "source": [
    "### Pipeline Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d45554-3efd-41ca-b5b0-8e68c3b7de7f",
   "metadata": {},
   "source": [
    "#### Data Preparation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33cd2722-e9fc-41e5-87fa-0d54425feec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data\n",
    "train_df, test_df = data_loading(\"train_full.csv\", \"test_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b047f1-562e-410c-a183-5745bdfca878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and transform(encodes) pathology labels\n",
    "train_df, test_df = labels_encoding(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beb30fc9-9569-40f2-9b0c-1095bafa070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'image_type', 'image_path', 'series_uid', 'subject_id',\n",
       "       'study_uid', 'breast_density', 'breast_side', 'image_view',\n",
       "       'abnormality_type', 'pathology', 'split', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abc2cb65-26ed-4787-bb71-d66af7b29445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1889 cases, 70.35 %\n",
      "Validation set: 234 cases, 8.72 %\n",
      "Test set: 562 cases, 20.93 %\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, val_data, test_data = split_data(train_df, test_df, 0.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e728c2a8-ebf2-4a14-a059-4d6c86eafbde",
   "metadata": {},
   "source": [
    "#### Iteration 1: Finding best preprocessing technique using custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a806ec0b-0461-449b-8323-b688fdf7e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables \n",
    "project_phase = \"P2\"\n",
    "epochs = 10\n",
    "options = ['apply_background_removal',\n",
    "           'apply_crop',\n",
    "           'apply_noise_reduction',\n",
    "           'apply_contrast_enhancement',\n",
    "           'apply_edge_enhancement',\n",
    "           'apply_lbp_texturizer']\n",
    "\n",
    "y_true = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c526edd3-7ae2-433e-8008-af0a6d8be615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create group of techniques to try\n",
    "techniques_groups = ablation(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2bc0106-c314-42ce-b165-fbc49d9cad65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Baseline Basic Preporcessing': {'apply_background_removal': False,\n",
       "  'apply_crop': False,\n",
       "  'apply_noise_reduction': False,\n",
       "  'apply_contrast_enhancement': False,\n",
       "  'apply_edge_enhancement': False,\n",
       "  'apply_lbp_texturizer': False},\n",
       " 'All Preporcessing Techniques': {'apply_background_removal': True,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Background removal': {'apply_background_removal': False,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Crop': {'apply_background_removal': True,\n",
       "  'apply_crop': False,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Noise reduction': {'apply_background_removal': True,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': False,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Contrast enhancement': {'apply_background_removal': True,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': False,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Edge enhancement': {'apply_background_removal': True,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': False,\n",
       "  'apply_lbp_texturizer': True},\n",
       " 'No Lbp texturizer': {'apply_background_removal': True,\n",
       "  'apply_crop': True,\n",
       "  'apply_noise_reduction': True,\n",
       "  'apply_contrast_enhancement': True,\n",
       "  'apply_edge_enhancement': True,\n",
       "  'apply_lbp_texturizer': False}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c0bbc4c-f53b-4325-83e7-abb68c056b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = train_data[:20]\n",
    "# val_data = val_data[:10]\n",
    "# test_data = test_data[:10]\n",
    "# y_true = y_true[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3c437b-5137-4802-9aa0-cc470523a1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Custom CNN 10 - Baseline Basic Preporcessing\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755827112.637563   69218 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1755827130.085777   69390 service.cc:148] XLA service 0x748be800a700 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1755827130.086879   69390 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-08-21 21:45:30.231696: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1755827130.584095   69390 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "E0000 00:00:1755827131.391814   69390 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/60\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12:37\u001b[0m 13s/step - accuracy: 0.5312 - auc: 0.4286 - loss: 0.6943 - precision: 0.4286 - recall: 0.2143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1755827135.242243   69390 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m366s\u001b[0m 6s/step - accuracy: 0.5212 - auc: 0.5108 - loss: 0.6929 - precision: 0.5262 - recall: 0.6163 - val_accuracy: 0.4744 - val_auc: 0.5311 - val_loss: 0.6929 - val_precision: 0.4800 - val_recall: 0.1983\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m434s\u001b[0m 7s/step - accuracy: 0.5058 - auc: 0.5438 - loss: 0.6910 - precision: 0.5333 - recall: 0.4365 - val_accuracy: 0.4786 - val_auc: 0.5094 - val_loss: 0.6956 - val_precision: 0.4977 - val_recall: 0.8926\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 7s/step - accuracy: 0.5656 - auc: 0.6038 - loss: 0.6794 - precision: 0.5611 - recall: 0.8085 - val_accuracy: 0.5000 - val_auc: 0.5249 - val_loss: 0.6910 - val_precision: 0.5110 - val_recall: 0.7686\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 6s/step - accuracy: 0.6071 - auc: 0.6368 - loss: 0.6733 - precision: 0.5863 - recall: 0.6975 - val_accuracy: 0.5342 - val_auc: 0.5505 - val_loss: 0.6914 - val_precision: 0.5526 - val_recall: 0.5207\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 7s/step - accuracy: 0.6041 - auc: 0.6482 - loss: 0.6619 - precision: 0.6258 - recall: 0.5783 - val_accuracy: 0.5342 - val_auc: 0.5278 - val_loss: 0.7100 - val_precision: 0.5337 - val_recall: 0.7851\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 6s/step - accuracy: 0.5992 - auc: 0.6506 - loss: 0.6573 - precision: 0.5897 - recall: 0.6723 - val_accuracy: 0.5256 - val_auc: 0.5516 - val_loss: 0.7056 - val_precision: 0.5312 - val_recall: 0.7025\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 6s/step - accuracy: 0.6226 - auc: 0.6816 - loss: 0.6411 - precision: 0.5933 - recall: 0.7311 - val_accuracy: 0.5256 - val_auc: 0.5612 - val_loss: 0.6978 - val_precision: 0.5342 - val_recall: 0.6446\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 6s/step - accuracy: 0.6425 - auc: 0.7034 - loss: 0.6241 - precision: 0.6389 - recall: 0.6654 - val_accuracy: 0.5299 - val_auc: 0.5579 - val_loss: 0.6976 - val_precision: 0.5385 - val_recall: 0.6364\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 6s/step - accuracy: 0.6710 - auc: 0.7322 - loss: 0.6128 - precision: 0.6774 - recall: 0.7054 - val_accuracy: 0.5513 - val_auc: 0.5593 - val_loss: 0.7101 - val_precision: 0.5690 - val_recall: 0.5455\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 8s/step - accuracy: 0.6786 - auc: 0.7395 - loss: 0.6047 - precision: 0.6937 - recall: 0.6636 - val_accuracy: 0.5342 - val_auc: 0.5523 - val_loss: 0.7178 - val_precision: 0.5545 - val_recall: 0.5041\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - All Preporcessing Techniques\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 8s/step - accuracy: 0.5273 - auc: 0.4832 - loss: 0.6929 - precision: 0.5273 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5177 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 6s/step - accuracy: 0.5079 - auc: 0.5036 - loss: 0.6933 - precision: 0.5074 - recall: 0.9956 - val_accuracy: 0.5171 - val_auc: 0.5381 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m400s\u001b[0m 7s/step - accuracy: 0.5336 - auc: 0.4816 - loss: 0.6925 - precision: 0.5336 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5365 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m361s\u001b[0m 6s/step - accuracy: 0.5217 - auc: 0.5291 - loss: 0.6923 - precision: 0.5217 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4636 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m440s\u001b[0m 7s/step - accuracy: 0.5236 - auc: 0.5278 - loss: 0.6921 - precision: 0.5236 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4800 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 6s/step - accuracy: 0.5245 - auc: 0.5418 - loss: 0.6924 - precision: 0.5207 - recall: 0.8865 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5177 - val_recall: 0.9669\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m363s\u001b[0m 6s/step - accuracy: 0.5027 - auc: 0.5145 - loss: 0.6927 - precision: 0.5048 - recall: 0.9442 - val_accuracy: 0.5171 - val_auc: 0.5028 - val_loss: 0.6933 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m448s\u001b[0m 7s/step - accuracy: 0.5271 - auc: 0.5022 - loss: 0.6920 - precision: 0.5270 - recall: 0.9997 - val_accuracy: 0.4829 - val_auc: 0.5070 - val_loss: 0.6930 - val_precision: 0.5000 - val_recall: 0.7851\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m480s\u001b[0m 8s/step - accuracy: 0.5248 - auc: 0.5128 - loss: 0.6927 - precision: 0.5174 - recall: 0.7026 - val_accuracy: 0.5171 - val_auc: 0.4965 - val_loss: 0.6943 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 7s/step - accuracy: 0.5244 - auc: 0.5208 - loss: 0.6921 - precision: 0.5218 - recall: 0.9136 - val_accuracy: 0.5171 - val_auc: 0.4926 - val_loss: 0.6939 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Background removal\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 8s/step - accuracy: 0.4923 - auc: 0.5000 - loss: 0.6932 - precision: 0.4503 - recall: 0.1888 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6930 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 7s/step - accuracy: 0.5132 - auc: 0.4952 - loss: 0.6929 - precision: 0.5132 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5330 - val_loss: 0.6925 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 8s/step - accuracy: 0.5268 - auc: 0.5137 - loss: 0.6925 - precision: 0.5268 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5196 - val_loss: 0.6923 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 6s/step - accuracy: 0.5125 - auc: 0.5193 - loss: 0.6927 - precision: 0.5110 - recall: 0.9817 - val_accuracy: 0.5171 - val_auc: 0.5366 - val_loss: 0.6923 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 7s/step - accuracy: 0.5259 - auc: 0.5322 - loss: 0.6915 - precision: 0.5259 - recall: 0.9999 - val_accuracy: 0.5214 - val_auc: 0.5103 - val_loss: 0.6920 - val_precision: 0.5204 - val_recall: 0.9504\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m423s\u001b[0m 7s/step - accuracy: 0.5419 - auc: 0.5400 - loss: 0.6917 - precision: 0.5293 - recall: 0.9274 - val_accuracy: 0.5171 - val_auc: 0.5264 - val_loss: 0.6942 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m497s\u001b[0m 8s/step - accuracy: 0.5345 - auc: 0.5212 - loss: 0.6910 - precision: 0.5326 - recall: 0.9932 - val_accuracy: 0.5385 - val_auc: 0.5400 - val_loss: 0.6912 - val_precision: 0.5330 - val_recall: 0.8678\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m386s\u001b[0m 6s/step - accuracy: 0.5491 - auc: 0.5581 - loss: 0.6898 - precision: 0.5353 - recall: 0.8011 - val_accuracy: 0.5342 - val_auc: 0.5266 - val_loss: 0.6912 - val_precision: 0.5353 - val_recall: 0.7521\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m337s\u001b[0m 6s/step - accuracy: 0.5690 - auc: 0.5537 - loss: 0.6891 - precision: 0.5605 - recall: 0.8525 - val_accuracy: 0.5427 - val_auc: 0.5298 - val_loss: 0.6903 - val_precision: 0.5372 - val_recall: 0.8347\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m345s\u001b[0m 6s/step - accuracy: 0.5726 - auc: 0.5965 - loss: 0.6855 - precision: 0.5591 - recall: 0.8096 - val_accuracy: 0.5385 - val_auc: 0.5327 - val_loss: 0.6898 - val_precision: 0.5330 - val_recall: 0.8678\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 7s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Crop\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 8s/step - accuracy: 0.5078 - auc: 0.5006 - loss: 0.6932 - precision: 0.2984 - recall: 0.1872 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m406s\u001b[0m 7s/step - accuracy: 0.5168 - auc: 0.4859 - loss: 0.6929 - precision: 0.5168 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4701 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 6s/step - accuracy: 0.5162 - auc: 0.4987 - loss: 0.6928 - precision: 0.5162 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m410s\u001b[0m 7s/step - accuracy: 0.4870 - auc: 0.4741 - loss: 0.6937 - precision: 0.4870 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 7s/step - accuracy: 0.5499 - auc: 0.4681 - loss: 0.6919 - precision: 0.5499 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m424s\u001b[0m 7s/step - accuracy: 0.5324 - auc: 0.4793 - loss: 0.6919 - precision: 0.5324 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m376s\u001b[0m 6s/step - accuracy: 0.5162 - auc: 0.4821 - loss: 0.6929 - precision: 0.5162 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 6s/step - accuracy: 0.5290 - auc: 0.4913 - loss: 0.6920 - precision: 0.5290 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4959 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 7s/step - accuracy: 0.5191 - auc: 0.5132 - loss: 0.6923 - precision: 0.5191 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m427s\u001b[0m 7s/step - accuracy: 0.5017 - auc: 0.4920 - loss: 0.6933 - precision: 0.5017 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 6s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Noise reduction\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 6s/step - accuracy: 0.4795 - auc: 0.5000 - loss: 0.6932 - precision: 0.4804 - recall: 0.6485 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m359s\u001b[0m 6s/step - accuracy: 0.5101 - auc: 0.5065 - loss: 0.6931 - precision: 0.5101 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 8s/step - accuracy: 0.5132 - auc: 0.4979 - loss: 0.6929 - precision: 0.5132 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4823 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m437s\u001b[0m 7s/step - accuracy: 0.5219 - auc: 0.4925 - loss: 0.6924 - precision: 0.5219 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m416s\u001b[0m 7s/step - accuracy: 0.5061 - auc: 0.5005 - loss: 0.6931 - precision: 0.5061 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m429s\u001b[0m 7s/step - accuracy: 0.5025 - auc: 0.5099 - loss: 0.6933 - precision: 0.5025 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 8s/step - accuracy: 0.5248 - auc: 0.4858 - loss: 0.6923 - precision: 0.5248 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 6s/step - accuracy: 0.5294 - auc: 0.4788 - loss: 0.6921 - precision: 0.5294 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m517s\u001b[0m 9s/step - accuracy: 0.5293 - auc: 0.5009 - loss: 0.6922 - precision: 0.5293 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 6s/step - accuracy: 0.4992 - auc: 0.5069 - loss: 0.6932 - precision: 0.4992 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 6s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Contrast enhancement\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 8s/step - accuracy: 0.5196 - auc: 0.4974 - loss: 0.6926 - precision: 0.5260 - recall: 0.9432 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 7s/step - accuracy: 0.5298 - auc: 0.5285 - loss: 0.6929 - precision: 0.5310 - recall: 0.9882 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6931 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 7s/step - accuracy: 0.5276 - auc: 0.4955 - loss: 0.6930 - precision: 0.5276 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m540s\u001b[0m 9s/step - accuracy: 0.5197 - auc: 0.4999 - loss: 0.6927 - precision: 0.5197 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 7s/step - accuracy: 0.5013 - auc: 0.4954 - loss: 0.6933 - precision: 0.5013 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m438s\u001b[0m 7s/step - accuracy: 0.5103 - auc: 0.4976 - loss: 0.6929 - precision: 0.5103 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5323 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 7s/step - accuracy: 0.5168 - auc: 0.5016 - loss: 0.6926 - precision: 0.5168 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5057 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m377s\u001b[0m 6s/step - accuracy: 0.5098 - auc: 0.5183 - loss: 0.6929 - precision: 0.5098 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4642 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 7s/step - accuracy: 0.5095 - auc: 0.4855 - loss: 0.6933 - precision: 0.5095 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m535s\u001b[0m 9s/step - accuracy: 0.5106 - auc: 0.4884 - loss: 0.6932 - precision: 0.5106 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4701 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 6s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Edge enhancement\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 7s/step - accuracy: 0.5088 - auc: 0.4873 - loss: 0.6930 - precision: 0.5237 - recall: 0.8325 - val_accuracy: 0.4829 - val_auc: 0.5000 - val_loss: 0.6932 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 7s/step - accuracy: 0.4871 - auc: 0.5000 - loss: 0.6932 - precision: 0.3236 - recall: 0.2566 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6931 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m395s\u001b[0m 7s/step - accuracy: 0.5252 - auc: 0.5000 - loss: 0.6931 - precision: 0.5252 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6930 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m447s\u001b[0m 7s/step - accuracy: 0.5167 - auc: 0.5027 - loss: 0.6929 - precision: 0.5167 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6930 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 6s/step - accuracy: 0.5162 - auc: 0.5029 - loss: 0.6930 - precision: 0.5162 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m364s\u001b[0m 6s/step - accuracy: 0.5137 - auc: 0.5000 - loss: 0.6929 - precision: 0.5137 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m401s\u001b[0m 7s/step - accuracy: 0.5126 - auc: 0.4941 - loss: 0.6931 - precision: 0.5126 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6930 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m512s\u001b[0m 9s/step - accuracy: 0.5294 - auc: 0.5000 - loss: 0.6928 - precision: 0.5294 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6926 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 8s/step - accuracy: 0.5118 - auc: 0.4767 - loss: 0.6930 - precision: 0.5118 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m538s\u001b[0m 9s/step - accuracy: 0.5193 - auc: 0.5008 - loss: 0.6927 - precision: 0.5193 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 7s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n",
      "Training Custom CNN 10 - No Lbp texturizer\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m563s\u001b[0m 9s/step - accuracy: 0.5026 - auc: 0.4884 - loss: 0.6931 - precision: 0.5118 - recall: 0.9428 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 8s/step - accuracy: 0.5117 - auc: 0.4542 - loss: 0.6937 - precision: 0.5117 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5003 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m398s\u001b[0m 7s/step - accuracy: 0.5116 - auc: 0.4947 - loss: 0.6931 - precision: 0.5116 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 7s/step - accuracy: 0.5069 - auc: 0.4866 - loss: 0.6936 - precision: 0.5069 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5003 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m412s\u001b[0m 7s/step - accuracy: 0.5292 - auc: 0.4823 - loss: 0.6917 - precision: 0.5292 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4838 - val_loss: 0.6928 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 7s/step - accuracy: 0.5288 - auc: 0.5144 - loss: 0.6914 - precision: 0.5288 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m348s\u001b[0m 6s/step - accuracy: 0.5255 - auc: 0.4887 - loss: 0.6924 - precision: 0.5255 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m473s\u001b[0m 8s/step - accuracy: 0.5260 - auc: 0.5008 - loss: 0.6918 - precision: 0.5260 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4686 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m365s\u001b[0m 6s/step - accuracy: 0.5204 - auc: 0.4941 - loss: 0.6925 - precision: 0.5204 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.5000 - val_loss: 0.6927 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 6s/step - accuracy: 0.5240 - auc: 0.4846 - loss: 0.6924 - precision: 0.5240 - recall: 1.0000 - val_accuracy: 0.5171 - val_auc: 0.4750 - val_loss: 0.6929 - val_precision: 0.5171 - val_recall: 1.0000\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/models_data.json \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate trough techniques groups for training a model with each group\n",
    "for technique_name, techniques in techniques_groups.items():\n",
    "    \n",
    "    # create model name\n",
    "    model_name = \"Custom CNN \" + str(epochs) + \" - \"+ technique_name\n",
    "    print(\"Training \" + model_name)\n",
    "    \n",
    "    # reset and clears variables before creating a new model \n",
    "    K.clear_session()\n",
    "    \n",
    "    # Create image iterators with preprocessing function for each set of preprocessing techniques \n",
    "    train_generator, val_generator, test_generator = image_iterators((train_data, val_data, test_data), \n",
    "                                                    is_resnet_vgg=False,\n",
    "                                                    preprocessing_techniques=techniques\n",
    "                                                  )\n",
    "    \n",
    "    # initiate model class\n",
    "    model_instance = Basic_Custom_CNN(input_shape=(256, 256, 1), num_classes=2, epochs=epochs)\n",
    "    \n",
    "    # create model architecture\n",
    "    model_instance.architecture()\n",
    "    \n",
    "    # train model\n",
    "    history = model_instance.train_model(train_generator, val_gen=val_generator)\n",
    "    \n",
    "    # save model and get path\n",
    "    name = model_name.lower().replace(\" \", \"_\") + \".keras\"\n",
    "    model_path = model_instance.save_model(models_directory=\"Models\", model_file=name)\n",
    "\n",
    "    # evaluate model by making predictions\n",
    "    evaluation = Evaluation(model_instance.get_model())\n",
    "    y_probs = evaluation.predict(test_generator)\n",
    "\n",
    "    # calculate metrics\n",
    "    metrics = evaluation.calculate_metrics(y_true, y_probs)\n",
    "\n",
    "    # get labels dictionary\n",
    "    y_labels = evaluation.get_labels()\n",
    "\n",
    "    # save data\n",
    "    save_data = Save_Data(file_name=\"models_data.json\", out_directory=\"Outputs\")\n",
    "    save_data.add_model_data(model_name, model_path, epochs, history, metrics, y_labels, project_phase, comments=\"\")\n",
    "    save_data.save_model_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e2a570-1f9d-479f-9e52-ac32c9498965",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d278c2-0f56-4ab8-99b1-1f8e9518e74e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688e067-e7e7-41d2-a305-b1267676e248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc37c08a-54ea-45a8-932f-80be70e61c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get predictions for the test set\n",
    "y_probs = evaluation.predict(test_generator)\n",
    "y_true = test_generator.labels\n",
    "\n",
    "# 2. Flatten predictions if sigmoid output\n",
    "y_probs = y_probs.ravel()\n",
    "\n",
    "# 3. Separate probabilities by class\n",
    "pos_probs = y_probs[y_true == 1]  # predicted probs for actual positives\n",
    "neg_probs = y_probs[y_true == 0]  # predicted probs for actual negatives\n",
    "\n",
    "# 4. Plot histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(neg_probs, bins=20, alpha=0.6, label=\"Class 0 (negatives)\")\n",
    "plt.hist(pos_probs, bins=20, alpha=0.6, label=\"Class 1 (positives)\")\n",
    "plt.axvline(0.5, color='red', linestyle='--', label=\"Decision threshold 0.5\")\n",
    "\n",
    "plt.xlabel(\"Predicted probability for class 1\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of predicted probabilities\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b751e66-c015-4ab6-964b-cad101ece0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454b1cf7-2733-43fc-9af4-75e2414a7cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63935e-eb98-4c40-aa9f-c4fc73349890",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b69c40-4480-4e45-8df1-83b5905cbefb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DQNvenv)",
   "language": "python",
   "name": "dqnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
