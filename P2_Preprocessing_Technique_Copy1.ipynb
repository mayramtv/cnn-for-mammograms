{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e99e55-341e-43da-8c1f-afee52bb0e3c",
   "metadata": {},
   "source": [
    "# Phase 2: Find Processing Techniques\n",
    "## Train and Visualize Custom CNN Models \n",
    "### Import Libraries and Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fd6e95-d1fa-48f2-bd0c-3bef4f9577fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbec7fd7-6fc7-4346-ae93-4d26542a2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions \n",
    "from Utils.preporcessing_utils import data_loading \n",
    "from Utils.preporcessing_utils import labels_encoding\n",
    "from Utils.preporcessing_utils import split_data\n",
    "from Utils.preporcessing_utils import ablation\n",
    "from Utils.preporcessing_utils_Copy1 import preprocess_locally\n",
    "from Utils.models_utils import Basic_Custom_CNN\n",
    "from Utils.evaluation_utils import Evaluation\n",
    "from Utils.visualizations_utils import Visualization\n",
    "from Utils.save_data_utils import Save_Data\n",
    "from Utils.run_model_utils import run_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc3f4e7-0e09-4cb6-a4dd-38af1616dba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import json \n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d45554-3efd-41ca-b5b0-8e68c3b7de7f",
   "metadata": {},
   "source": [
    "### Data Preparation and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cd2722-e9fc-41e5-87fa-0d54425feec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data\n",
    "train_df, test_df = data_loading(\"train_full.csv\", \"test_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5b047f1-562e-410c-a183-5745bdfca878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and transform(encodes) pathology labels\n",
    "train_df, test_df = labels_encoding(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb30fc9-9569-40f2-9b0c-1095bafa070d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'image_type', 'image_path', 'series_uid', 'subject_id',\n",
       "       'study_uid', 'breast_density', 'breast_side', 'image_view',\n",
       "       'abnormality_type', 'pathology', 'split', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc2cb65-26ed-4787-bb71-d66af7b29445",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1889 cases, 70.35 %\n",
      "Validation set: 234 cases, 8.72 %\n",
      "Test set: 562 cases, 20.93 %\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, val_data, test_data = split_data(train_df, test_df, 0.11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad2f266a-4a45-461f-b758-7afbf2a412ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_10 = train_data[:10]\n",
    "val_data_10 = val_data[:10]\n",
    "test_data_10 = test_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3891839-3d10-428b-90a1-708b44d7d6fe",
   "metadata": {},
   "source": [
    "### Create Preprocessing Groups by Using Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a806ec0b-0461-449b-8323-b688fdf7e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create groups using ablation\n",
    "options = ['apply_background_removal',\n",
    "           'apply_crop',\n",
    "           'apply_noise_reduction',\n",
    "           'apply_contrast_enhancement',\n",
    "           'apply_edge_enhancement',\n",
    "           'apply_lbp_texturizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c526edd3-7ae2-433e-8008-af0a6d8be615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create group of techniques to try\n",
    "techniques_groups = ablation(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2bc0106-c314-42ce-b165-fbc49d9cad65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apply_background_removal': False,\n",
       " 'apply_crop': False,\n",
       " 'apply_noise_reduction': False,\n",
       " 'apply_contrast_enhancement': False,\n",
       " 'apply_edge_enhancement': False,\n",
       " 'apply_lbp_texturizer': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques_groups[\"Baseline Basic Preporcessing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b7baff-5b70-4199-96ae-3ecec15cfab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae5f9ab-36ea-49f4-a63c-97ac74620674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d382744-ad98-4bf3-a5f3-9fb3f1ccb643",
   "metadata": {},
   "source": [
    "### Baseline model with ImageGenerator\n",
    "This models uses the custom CNN with 10 epochs and the tf.ImageDataGenerator image iterator. <br>\n",
    "The preprocessing was done Using the ImageGenerator during training. <br><br> \n",
    "The following model call will not run because it is not connected the image iterator funtion anymore becaue the function was updated. <br> \n",
    "This output was kept for reference only. \n",
    "If needed, please see **run_model_utils.py** for running instructions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1b36cd-799d-4084-8f7c-0c3d56e62243",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Custom10 - Baseline Basic Preporcessing - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756665577.419168   33308 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1756665588.778667   33462 service.cc:148] XLA service 0x739a240099a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1756665588.778734   33462 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2025-08-31 14:39:48.797362: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1756665588.887976   33462 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "E0000 00:00:1756665591.911711   33462 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1756665592.022091   33462 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/60\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:03\u001b[0m 9s/step - accuracy: 0.5000 - loss: 0.6927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1756665593.272752   33462 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 5s/step - accuracy: 0.5005 - loss: 0.6944 - val_accuracy: 0.5171 - val_loss: 0.6937\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 5s/step - accuracy: 0.5577 - loss: 0.6844 - val_accuracy: 0.4829 - val_loss: 0.6928\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m282s\u001b[0m 5s/step - accuracy: 0.5870 - loss: 0.6787 - val_accuracy: 0.5556 - val_loss: 0.6932\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 5s/step - accuracy: 0.5856 - loss: 0.6661 - val_accuracy: 0.5256 - val_loss: 0.6838\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 5s/step - accuracy: 0.5976 - loss: 0.6517 - val_accuracy: 0.5342 - val_loss: 0.6886\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 5s/step - accuracy: 0.6046 - loss: 0.6471 - val_accuracy: 0.5171 - val_loss: 0.6895\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 5s/step - accuracy: 0.6418 - loss: 0.6359 - val_accuracy: 0.5342 - val_loss: 0.6880\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 5s/step - accuracy: 0.6388 - loss: 0.6237 - val_accuracy: 0.5299 - val_loss: 0.6959\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 5s/step - accuracy: 0.6656 - loss: 0.6135 - val_accuracy: 0.5256 - val_loss: 0.7035\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 4s/step - accuracy: 0.6639 - loss: 0.6046 - val_accuracy: 0.5385 - val_loss: 0.7134\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_baseline_basic_preporcessing_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - All Preporcessing Techniques - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 5s/step - accuracy: 0.4986 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 5s/step - accuracy: 0.5142 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 5s/step - accuracy: 0.5133 - loss: 0.6931 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 5s/step - accuracy: 0.4939 - loss: 0.6933 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 5s/step - accuracy: 0.5137 - loss: 0.6931 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 5s/step - accuracy: 0.5139 - loss: 0.6931 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 5s/step - accuracy: 0.5306 - loss: 0.6919 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 5s/step - accuracy: 0.5012 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 5s/step - accuracy: 0.4985 - loss: 0.6934 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 5s/step - accuracy: 0.5261 - loss: 0.6924 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_all_preporcessing_techniques_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Background removal - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 5s/step - accuracy: 0.5357 - loss: 0.6929 - val_accuracy: 0.5171 - val_loss: 0.6925\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 5s/step - accuracy: 0.5192 - loss: 0.6926 - val_accuracy: 0.5171 - val_loss: 0.6924\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 5s/step - accuracy: 0.5188 - loss: 0.6925 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 5s/step - accuracy: 0.5273 - loss: 0.6928 - val_accuracy: 0.5385 - val_loss: 0.6920\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 5s/step - accuracy: 0.5549 - loss: 0.6906 - val_accuracy: 0.5385 - val_loss: 0.6911\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 5s/step - accuracy: 0.5447 - loss: 0.6890 - val_accuracy: 0.5470 - val_loss: 0.6902\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 5s/step - accuracy: 0.5715 - loss: 0.6829 - val_accuracy: 0.5385 - val_loss: 0.6909\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 5s/step - accuracy: 0.5711 - loss: 0.6790 - val_accuracy: 0.5171 - val_loss: 0.6951\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m316s\u001b[0m 5s/step - accuracy: 0.5660 - loss: 0.6894 - val_accuracy: 0.4872 - val_loss: 0.6943\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 5s/step - accuracy: 0.5308 - loss: 0.6866 - val_accuracy: 0.5342 - val_loss: 0.6966\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_background_removal_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Crop - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 6s/step - accuracy: 0.4780 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 5s/step - accuracy: 0.5141 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 5s/step - accuracy: 0.5071 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 5s/step - accuracy: 0.5166 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 5s/step - accuracy: 0.4996 - loss: 0.6945 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 5s/step - accuracy: 0.5092 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 5s/step - accuracy: 0.5028 - loss: 0.6933 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 5s/step - accuracy: 0.5192 - loss: 0.6925 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 5s/step - accuracy: 0.5147 - loss: 0.6928 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 5s/step - accuracy: 0.5240 - loss: 0.6923 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_crop_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Noise reduction - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 5s/step - accuracy: 0.5202 - loss: 0.6935 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 5s/step - accuracy: 0.5305 - loss: 0.6922 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 5s/step - accuracy: 0.5254 - loss: 0.6920 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 5s/step - accuracy: 0.5422 - loss: 0.6906 - val_accuracy: 0.5171 - val_loss: 0.6931\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 5s/step - accuracy: 0.5264 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m299s\u001b[0m 5s/step - accuracy: 0.5343 - loss: 0.6926 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m326s\u001b[0m 5s/step - accuracy: 0.5184 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 5s/step - accuracy: 0.5022 - loss: 0.6933 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 5s/step - accuracy: 0.5033 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m276s\u001b[0m 5s/step - accuracy: 0.5167 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 4s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_noise_reduction_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Contrast enhancement - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 5s/step - accuracy: 0.5272 - loss: 0.6929 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 5s/step - accuracy: 0.5092 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6931\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 5s/step - accuracy: 0.5023 - loss: 0.6931 - val_accuracy: 0.5171 - val_loss: 0.6931\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m280s\u001b[0m 5s/step - accuracy: 0.5022 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 5s/step - accuracy: 0.5224 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6930\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 5s/step - accuracy: 0.5211 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6930\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 5s/step - accuracy: 0.5177 - loss: 0.6930 - val_accuracy: 0.5171 - val_loss: 0.6933\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 5s/step - accuracy: 0.5164 - loss: 0.6926 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 5s/step - accuracy: 0.5331 - loss: 0.6907 - val_accuracy: 0.5085 - val_loss: 0.6933\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 4s/step - accuracy: 0.5419 - loss: 0.6921 - val_accuracy: 0.5043 - val_loss: 0.6930\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_contrast_enhancement_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Edge enhancement - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 5s/step - accuracy: 0.4842 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6930\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m344s\u001b[0m 6s/step - accuracy: 0.4977 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6930\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 6s/step - accuracy: 0.5133 - loss: 0.6929 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 5s/step - accuracy: 0.5165 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6930\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 5s/step - accuracy: 0.5216 - loss: 0.6929 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m311s\u001b[0m 5s/step - accuracy: 0.5262 - loss: 0.6921 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 5s/step - accuracy: 0.4927 - loss: 0.6934 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 5s/step - accuracy: 0.5159 - loss: 0.6928 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 5s/step - accuracy: 0.5251 - loss: 0.6923 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m302s\u001b[0m 5s/step - accuracy: 0.5050 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_edge_enhancement_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n",
      "Training Custom10 - No Lbp texturizer - Baseline(tf.ImageDataGenerator)\n",
      "Found 1889 validated image filenames.\n",
      "Found 234 validated image filenames.\n",
      "Found 562 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 5s/step - accuracy: 0.4873 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6931\n",
      "Epoch 2/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 5s/step - accuracy: 0.4752 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6929\n",
      "Epoch 3/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m322s\u001b[0m 5s/step - accuracy: 0.5305 - loss: 0.6925 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 4/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 5s/step - accuracy: 0.5188 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 5/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 5s/step - accuracy: 0.5194 - loss: 0.6927 - val_accuracy: 0.5171 - val_loss: 0.6928\n",
      "Epoch 6/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 5s/step - accuracy: 0.5261 - loss: 0.6924 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "Epoch 7/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 5s/step - accuracy: 0.5055 - loss: 0.6933 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 8/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 5s/step - accuracy: 0.4969 - loss: 0.6936 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 9/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m304s\u001b[0m 5s/step - accuracy: 0.5232 - loss: 0.6924 - val_accuracy: 0.5171 - val_loss: 0.6927\n",
      "Epoch 10/10\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 5s/step - accuracy: 0.5013 - loss: 0.6932 - val_accuracy: 0.5171 - val_loss: 0.6926\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 5s/step\n",
      "[INFO]Models data is saved to Outputs/custom10_-_no_lbp_texturizer_-_baseline(tf.imagedatagenerator).json \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# runs baseline model with an image dataset generator \n",
    "run_model((train_data, val_data, test_data), techniques_groups, epochs=10, change=\"Baseline(tf.ImageDataGenerator)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4980d8-0be1-423b-8d11-37faf7c9f01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82c82b5-a6d5-488c-93df-9d816c0f8797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a4312e8-42f5-43b1-ba0a-ab55fef8c035",
   "metadata": {},
   "source": [
    "## Search Preprocessing Technique Using Custom CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e41365eb-bc07-4460-8c33-2b064f49f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture_settings = {\n",
    "    \"iteration 1\": {\n",
    "        \"epochs\":10,\n",
    "        \"layers\": [32, 64, 128],\n",
    "        \"activation\": 'relu',\n",
    "        \"dropout\": None\n",
    "    },\n",
    "    \"iteration 1\": {\n",
    "        \"epochs\":10,\n",
    "        \"layers\": [32, 64, 128],\n",
    "        \"activation\": 'relu',\n",
    "        \"dropout\": 0.2\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8de15b1-7464-422a-8df8-3bcb9a58fa9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ee96dcb-0bf7-4999-aec0-762c46fd1267",
   "metadata": {},
   "source": [
    "### Baseline model with Image tf.Dataset Generator\n",
    "This models uses the custom CNN with 10 epochs and the tf.Dataset image iterator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fa0fd50-2b1c-40ba-bc90-cc0c90b778bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Custom10 - Baseline Basic Preporcessing - Baseline(tf.Dataset)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757188448.683965   96984 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5563 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">115200</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">115,201</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ rescaling (\u001b[38;5;33mRescaling\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m256\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m115200\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │       \u001b[38;5;34m115,201\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,873</span> (812.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,873\u001b[0m (812.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,873</span> (812.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,873\u001b[0m (812.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 15:54:10.649013: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.323499822612107717622923360172400112625_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649070: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649110: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.245398881011643929625650254320825052239_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649127: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.390769625211923456632942206872638891773_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649166: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.277995323110620578302169391360050541146_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649197: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.230147294010770633909173559510996309816_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649253: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.163779167012604986616722705001823633288_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649293: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.291104348113557290314520836302969063188_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649342: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.426409412812409195109420025761008801742_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649362: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.129626647210306079835703676703413955342_full.png; No such file or directory\n",
      "2025-09-06 15:54:10.649651: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n",
      "\t [[{{node ReadFile}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "2025-09-06 15:54:10.649693: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: NOT_FOUND: /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n",
      "\t [[{{node ReadFile}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[IteratorGetNext/_2]]\n",
      "2025-09-06 15:54:10.649719: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 67669478090145593\n",
      "2025-09-06 15:54:10.649739: I tensorflow/core/framework/local_rendezvous.cc:424] Local rendezvous recv item cancelled. Key hash: 14141776615131231934\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_1806]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# runs baseline model with an image dataset generator \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data_10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data_10\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtechniques_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_preprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchange\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBaseline(tf.Dataset)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/FP/cnn-for-mammograms/Utils/run_model_utils.py:48\u001b[0m, in \u001b[0;36mrun_model\u001b[0;34m(data_sets, techniques_groups, with_preprocess, epochs, project_phase, change)\u001b[0m\n\u001b[1;32m     45\u001b[0m model_instance\u001b[38;5;241m.\u001b[39mget_model()\u001b[38;5;241m.\u001b[39msummary()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mval_gen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# save model and get path\u001b[39;00m\n\u001b[1;32m     52\u001b[0m keras_name \u001b[38;5;241m=\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/FP/cnn-for-mammograms/Utils/models_utils.py:51\u001b[0m, in \u001b[0;36mBasic_Custom_CNN.train_model\u001b[0;34m(self, train_gen, val_gen)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to initialize model using architecture() before training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# fit data to model\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history\u001b[38;5;241m.\u001b[39mhistory\n",
      "File \u001b[0;32m/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/mnt/c/Users/mayra/OneDrive/Documents/MayraCSc/AI/DQNvenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Graph execution error:\n\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\nDetected at node ReadFile defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) NOT_FOUND:  /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) NOT_FOUND:  /Preprocessed_Images/custom_baseline_basic_preporcessing/1.3.6.1.4.1.9590.100.1.2.81539754211044334004264470202062485069_full.png; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_1806]"
     ]
    }
   ],
   "source": [
    "# runs baseline model with an image dataset generator \n",
    "run_model((train_10, val_data_10, test_data_10), techniques_groups, with_preprocess=False, epochs=10, change=\"Baseline(tf.Dataset)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f22f89f-04f3-4410-9a33-75b0c35f33a2",
   "metadata": {},
   "source": [
    "## Add Layers to Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1b93cc-5cc5-42f2-87fe-20c517063f53",
   "metadata": {},
   "source": [
    "## Add Dropout "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c75ad4-4616-4013-85ef-8a08e5a327b6",
   "metadata": {},
   "source": [
    "### Iteration 1: Basic CNN Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af15e66-6e97-4778-939a-b002dd8139f4",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626235e-26d9-4bcb-a359-a12d27a5b2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ef2c0-3211-479d-825f-e5924affcc09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766dd120-9f54-4e36-9eba-1cf86264eda4",
   "metadata": {},
   "source": [
    "#### Visualization and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec97767-c968-4652-b91b-5adc38325dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve models data \n",
    "with open('../Outputs/models_data.json', 'r') as file:\n",
    "        data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fbe17-b74c-4fb3-b6b0-21f59d064319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shows confusion matrix\n",
    "vis = Visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899de923-a071-4da6-8d80-7cee5ad2c5b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0683c943-f16e-408d-861e-d0c5f5c0dea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8fb9fb-d660-43af-a112-3a595bb62a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DQNvenv)",
   "language": "python",
   "name": "dqnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
