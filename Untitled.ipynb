{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b529a560-c6fb-4025-894a-80693a87c5ef",
   "metadata": {},
   "source": [
    "# Preprocess image and saves it locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efb6f37d-5c2d-4d1a-b064-86ebc15c7da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb67da6b-103e-4eeb-97a6-43548afd63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import functions \n",
    "# from Utils.preporcessing_utils import data_loading \n",
    "# from Utils.preporcessing_utils import labels_encoding\n",
    "# from Utils.preporcessing_utils import split_data\n",
    "# from Utils.preporcessing_utils import ablation\n",
    "# from Utils.preporcessing_utils import image_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9e6db44-7652-4626-b5cc-b44dce417971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-05 19:45:38.485454: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-05 19:45:39.420905: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1757115939.660472   79971 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1757115939.711003   79971 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-09-05 19:45:40.320989: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pywt\n",
    "from functools import partial\n",
    "from skimage.restoration import estimate_sigma\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import local_binary_pattern\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "# Preprocessing techniques to try\n",
    "\n",
    "def image_preprocessing(image, \n",
    "                       preprocessing_techniques,\n",
    "                       is_resnet_vgg=False,\n",
    "                       custom_cnn_size=256, \n",
    "                       resnet_vgg_size=224):\n",
    "    '''\n",
    "    Preprocessed an image based in the techniques passed as argument.\n",
    "    Parameters:\n",
    "        - image for preprocessing\n",
    "        - preprocessing_techniques to be applied\n",
    "        - is_resnet_vgg is True or False depending on type of model\n",
    "        - custom_cnn_size is the size for the Custom CNN model input image to be resized\n",
    "        - resnet_vgg_size is the size for the ResNet/VGG models input image to be resized\"\n",
    "    '''\n",
    "        \n",
    "    def convert_uint8(img):\n",
    "        '''Convert image to intiger to work with cv2'''\n",
    "        if img.dtype != np.uint8:\n",
    "            img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX)\n",
    "            img = img.astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    def expand_dim(img):\n",
    "        '''Expand dimentions if channel is not added'''\n",
    "        if not is_resnet_vgg and img.ndim == 2:\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "        elif is_resnet_vgg and img.ndim == 2:\n",
    "            img = np.stack([img, img, img], axis=-1)\n",
    "        elif is_resnet_vgg and img.ndim == 3 and img.shape[-1] == 1:\n",
    "            img = np.repeat(img, 3, axis=-1)\n",
    "        return img\n",
    "            \n",
    "\n",
    "    def background_removal(image):\n",
    "        '''\n",
    "            Removes edge of whole image, blur to find Otsu threshold, finds closed mask, \n",
    "            find the largest connected region and generate Otsu mask to remove background and leave only breast\n",
    "        '''\n",
    "        \n",
    "        # normalize image and convert to UINT8 if needed\n",
    "        image = convert_uint8(image)\n",
    "        image = expand_dim(image)\n",
    "\n",
    "        \n",
    "        # resize to remove a contour of the whole image to remove some of the marks of x-rays that are not the breast\n",
    "        height, width = image.shape[:2]\n",
    "        image = image[45:height-45, 45:width-45, :]\n",
    "\n",
    "    \n",
    "        # smooth image\n",
    "        blur_img = cv2.GaussianBlur(image, (5,5), 0)\n",
    "    \n",
    "        # gets Otsu threshold \n",
    "        _, thresh = cv2.threshold(blur_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) \n",
    "    \n",
    "        # apply morphological closing to make sure parts of the breast are not removed \n",
    "        kernel = np.ones((15, 15), np.uint8) \n",
    "        closed_img = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel)\n",
    "    \n",
    "        # Identify connecting regions for each pixel edge and corner (8) of binary image \n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(closed_img, connectivity=8)\n",
    "    \n",
    "        # find the largest component that is connected\n",
    "        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "        \n",
    "        # generate mask of black background\n",
    "        mask = (labels == largest_label).astype(np.uint8) * 255\n",
    "        breast_img = cv2.bitwise_and(image, image, mask=mask)\n",
    "\n",
    "        # expand dimentions\n",
    "        breast_img = expand_dim(breast_img)\n",
    "    \n",
    "        return closed_img, image, mask, breast_img\n",
    "\n",
    "    \n",
    "\n",
    "    def crop(image, breast_mask=None):    \n",
    "        ''' Find contours of breast image using the mask.''' \n",
    "        \n",
    "        # normalize image and convert to UINT8 if needed\n",
    "        image = convert_uint8(img)\n",
    "        if breast_mask is not None:\n",
    "            breast_mask = convert_uint8(breast_mask)\n",
    "        \n",
    "        # - RETR_EXTERNAL: defines only external countour of the biggest section, \n",
    "        # - CHAIN_APPROX_SIMPLE: saves only non redundant and the simplest points of the countour \n",
    "        # source: https://medium.com/analytics-vidhya/opencv-findcontours-detailed-guide-692ee19eeb18\n",
    "        if breast_mask is None:\n",
    "            contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        else:\n",
    "            contours, _ = cv2.findContours(breast_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # checks for non-countour\n",
    "        if len(contours) == 0:\n",
    "            return image\n",
    "    \n",
    "        # find the countour area\n",
    "        # source: https://docs.opencv.org/3.4/dd/d49/tutorial_py_contour_features.html\n",
    "        area = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # find the bounding box\n",
    "        x, y, w, h = cv2.boundingRect(area)\n",
    "    \n",
    "        # crops image using bounding box\n",
    "        cropped = image[y:y+h, x:x+w]\n",
    "        \n",
    "        return cropped       \n",
    "    \n",
    "     \n",
    "    def noise_reduction(image):\n",
    "        '''\n",
    "        Noise removal using Wavelet with Soft Otsu Threshold: \n",
    "           - calculating coefficients of approximation and detail\n",
    "           - calculating sigma value\n",
    "           - calculating and applying soft thresholding value to coefficients \n",
    "           - and reconstructing image using coefficinets\n",
    "        '''\n",
    "        \n",
    "        # convert to float\n",
    "        image = img.astype(np.float32)\n",
    "        \n",
    "        # calculate sigma of the detail coefficients    \n",
    "        if not is_resnet_vgg:\n",
    "            # adjust channels\n",
    "            if image.ndim == 3 and image.shape[-1] == 1:\n",
    "                image = np.squeeze(image)  \n",
    "            sigma = estimate_sigma(image, channel_axis=None)\n",
    "        else:\n",
    "            sigma = estimate_sigma(image, channel_axis=-1)\n",
    "\n",
    "        # Calculate coefficients for the image using wavedec2 for 2D (image) decomposition \n",
    "        # using the  Daubechies wavelet db1 (Haar wavelet of interval of 0-1)  \n",
    "        init_coeffs = pywt.wavedec2(image, wavelet=\"db1\", level=2)\n",
    "            \n",
    "    \n",
    "        # calculate initial threshold\n",
    "        init_threshold = sigma * np.sqrt(2 * np.log2(image.size))\n",
    "    \n",
    "        # iterate through detail coefficients to apply threshhold function using the initial threshold\n",
    "        new_coeffs = [init_coeffs[0]]\n",
    "    \n",
    "        for level in init_coeffs[1:]:\n",
    "            tuple_vals = tuple(pywt.threshold(detail, init_threshold, mode='soft') for detail in level)\n",
    "            new_coeffs.append(tuple_vals)\n",
    "    \n",
    "        # Reconstruct image using waverec2\n",
    "        denoised_img = pywt.waverec2(new_coeffs, wavelet=\"db1\")\n",
    "        reconstructed = denoised_img[:image.shape[0], :image.shape[1]]\n",
    "        \n",
    "        # expand dimentions\n",
    "        reconstructed = expand_dim(reconstructed)\n",
    "        reconstructed = reconstructed.astype(np.float32)\n",
    "        \n",
    "        # clip values between 0-255\n",
    "        reconstructed = np.clip(reconstructed, 0, 255).astype(np.float32)\n",
    "        \n",
    "        return reconstructed\n",
    "\n",
    "    \n",
    "\n",
    "    def contrast_enhancement(image):\n",
    "        '''\n",
    "        Distribute gray peaks of image\n",
    "        '''\n",
    "        # code provenance \n",
    "        # https://docs.opencv.org/4.x/d5/daf/tutorial_py_histogram_equalization.html\n",
    "        \n",
    "        # normalize image and convert to UINT8 if needed\n",
    "        image = convert_uint8(img)\n",
    "    \n",
    "        # verify the image is in gray scale\n",
    "        if len(image.shape) == 3 and image.shape[-1] == 3:  # If color image\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # initialize CLAHE and apply to image\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "        clh_image = clahe.apply(image)\n",
    "\n",
    "        # expand dimentions\n",
    "        clh_image = expand_dim(clh_image)\n",
    "        \n",
    "        return clh_image\n",
    "\n",
    "    \n",
    "    \n",
    "    def edge_enhancement(guide_image, input_image, radius=8, epsilon = 0.0002):    \n",
    "        '''\n",
    "            Uses guided filter to smooth image and keep edges sharp. \n",
    "            Uses input image to be filter and as guide for pixel smoothness. Parameters are:\n",
    "            - guided image\n",
    "            - input image\n",
    "            - kernel radius\n",
    "            - regularization value\n",
    "        '''\n",
    "    \n",
    "        # Normalize value\n",
    "        guide_image = guide_image.astype(np.float32) / 255.0\n",
    "        input_image = input_image.astype(np.float32) / 255.0\n",
    "        \n",
    "        # compute local mean: guidance and input image\n",
    "        mean_guide = cv2.boxFilter(guide_image, -1, (radius, radius)) # depth is -1 for source depth\n",
    "        mean_input = cv2.boxFilter(input_image, -1, (radius, radius)) # depth is -1 for source depth\n",
    "        mean_g_i = cv2.boxFilter(guide_image * input_image, -1, (radius, radius)) # mean of product of both images\n",
    "        mean_g_g = cv2.boxFilter(guide_image * guide_image, -1, (radius, radius))# loacl mean of product of guide image\n",
    "        \n",
    "        # calculate variance \n",
    "        covar_g_i = mean_g_i - mean_guide * mean_input # of the product of loacl means of both images\n",
    "        var_g = mean_g_g - mean_guide * mean_guide # of guidance image \n",
    "    \n",
    "        # calculate coefficinets using means , covariance and variance\n",
    "        alpha = covar_g_i / (var_g + epsilon)\n",
    "        beta = mean_input - alpha * mean_guide\n",
    "    \n",
    "        # calculate mean of coefficinets\n",
    "        mean_alpha = cv2.boxFilter(alpha, -1, (radius, radius))\n",
    "        mean_beta = cv2.boxFilter(beta, -1, (radius, radius))\n",
    "        mean_alpha = np.expand_dims(mean_alpha, axis=-1)\n",
    "        mean_beta = np.expand_dims(mean_beta, axis=-1)\n",
    "    \n",
    "        # apply mean of alpha to the guide image and add mean of beta\n",
    "        filtered = mean_alpha * guide_image + mean_beta\n",
    "    \n",
    "        # convert back to intigers\n",
    "        filtered = (filtered * 255).astype(np.uint8)\n",
    "    \n",
    "        return filtered\n",
    "\n",
    "    \n",
    "    \n",
    "    def lbp_texturizer(image, n_points=8, radius=1):\n",
    "        '''Calculate the local binary pattern of an image: checks for sourronding points of the kernel\n",
    "            and gives a binary value depending if bigger or smaller than the center point. Parameters are:\n",
    "            - image\n",
    "            - number of points around center point\n",
    "            - radius of kernel\n",
    "        '''\n",
    "        # normalize image and convert to UINT8 if needed\n",
    "        image = convert_uint8(img)\n",
    "        \n",
    "        # convert to gray scale\n",
    "        if image.ndim == 3 and image.shape[-1] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # removes 3rd channel\n",
    "        if img.ndim == 3 and image.shape[-1] == 1:  # If color image\n",
    "            image = np.squeeze(image)\n",
    "    \n",
    "        # calculate local binary pattern\n",
    "        lbp = local_binary_pattern(image, n_points, radius, method=\"uniform\")\n",
    "    \n",
    "        # normalize image for visualization\n",
    "        lbp_img = np.uint8(255 * (lbp - lbp.min()) / (lbp.max() - lbp.min()))\n",
    "\n",
    "        # expand dimentions\n",
    "        lbp_img = expand_dim(lbp_img)\n",
    "    \n",
    "        return lbp_img\n",
    "\n",
    "    def resize(img, is_resnet_vgg=False, custom_cnn_size=256, resnet_vgg_size=224, is_lbp=False):\n",
    "        \"\"\"\n",
    "        Resize an image to fit a custom CNN, ResNet, or VGG model using TensorFlow ops.\n",
    "        Maintains aspect ratio, pads with zeros, and expands channels if needed.\n",
    "        \"\"\"\n",
    "    \n",
    "        # target size\n",
    "        target_size = resnet_vgg_size if is_resnet_vgg else custom_cnn_size\n",
    "    \n",
    "        # get current shape\n",
    "        shape = tf.shape(img)\n",
    "        height, width = shape[0], shape[1]\n",
    "\n",
    "        # make sure there is not division by zero\n",
    "        max_dim = tf.maximum(tf.maximum(height, width), 1) \n",
    "    \n",
    "        # compute resize ratio\n",
    "        ratio = tf.cast(target_size, tf.float32) / tf.cast(tf.maximum(height, width), tf.float32)\n",
    "        new_height = tf.cast(tf.round(tf.cast(height, tf.float32) * ratio), tf.int32)\n",
    "        new_width = tf.cast(tf.round(tf.cast(width, tf.float32) * ratio), tf.int32)\n",
    "    \n",
    "        # resize with bilinear interpolation (similar to cv2.INTER_AREA)\n",
    "        resized = tf.image.resize(img, [new_height, new_width], method=\"bilinear\")\n",
    "    \n",
    "        # pad to target_size Ã— target_size\n",
    "        pad_height = tf.maximum(target_size - new_height, 0)\n",
    "        pad_width = tf.maximum(target_size - new_width, 0)\n",
    "        top = pad_height // 2\n",
    "        bottom = pad_height - top\n",
    "        left = pad_width // 2\n",
    "        right = pad_width - left\n",
    "    \n",
    "        padded = tf.pad(resized, [[top, bottom], [left, right], [0, 0]], constant_values=0.0)\n",
    "    \n",
    "        # make sure shape is correct\n",
    "        padded = tf.image.resize_with_crop_or_pad(padded, target_size, target_size)\n",
    "    \n",
    "        # if custom CNN expects grayscale channel\n",
    "        if not is_resnet_vgg and not is_lbp and len(padded.shape) == 2:\n",
    "            padded = tf.expand_dims(padded, axis=-1)\n",
    "    \n",
    "        return padded\n",
    "\n",
    "\n",
    "    img = image\n",
    "    breast_mask = None\n",
    "    is_lbp = False\n",
    "    \n",
    "    if not is_resnet_vgg and img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "    \n",
    "    if preprocessing_techniques[\"apply_background_removal\"] == True:\n",
    "        _, _, breast_mask, img = background_removal(img)\n",
    "\n",
    "    if preprocessing_techniques[\"apply_crop\"] == True:\n",
    "        img = crop(img, breast_mask)\n",
    "\n",
    "    if preprocessing_techniques[\"apply_noise_reduction\"] == True:\n",
    "        img = noise_reduction(img)\n",
    "\n",
    "    if preprocessing_techniques[\"apply_contrast_enhancement\"] == True:\n",
    "        img = contrast_enhancement(img)\n",
    "\n",
    "    if preprocessing_techniques[\"apply_edge_enhancement\"] == True:\n",
    "        img = edge_enhancement(img, img)\n",
    "\n",
    "    if preprocessing_techniques[\"apply_lbp_texturizer\"] == True:\n",
    "        img = lbp_texturizer(img)\n",
    "        is_lbp=True\n",
    "\n",
    "    img = resize(img, \n",
    "                 is_resnet_vgg=is_resnet_vgg, \n",
    "                 custom_cnn_size=custom_cnn_size, \n",
    "                 resnet_vgg_size=resnet_vgg_size, \n",
    "                 is_lbp=is_lbp)\n",
    "    # check for correct size\n",
    "    if is_resnet_vgg:\n",
    "        corr_size = resnet_vgg_size\n",
    "        chan = 3\n",
    "    else:\n",
    "        corr_size = custom_cnn_size\n",
    "        chan = 1\n",
    "\n",
    "    assert img.shape[:2] == (corr_size, corr_size), f\"Incorrect shape {img.shape}, expected ({corr_size}, {corr_size}, {chan})\"\n",
    "        \n",
    "    img = img.numpy().astype(\"float32\")\n",
    "    # convert image to tensorflow image\n",
    "    # img = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "\n",
    "    tf.print(\"DEBUG 2:\",\n",
    "             \"shape =\", tf.shape(img),\n",
    "             \"dtype =\", img.dtype,\n",
    "             \"min =\", tf.reduce_min(img),\n",
    "             \"max =\", tf.reduce_max(img))\n",
    "    return img\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c71c6c49-5d67-48cd-8581-1a8c3f4e11c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and encoding\n",
    "def data_loading(train_file, test_file):\n",
    "    '''Loads data using the files names'''\n",
    "    \n",
    "    base_path = \"CBIS-DDSM_Clean_Data/\"\n",
    "    train = pd.read_csv(base_path + train_file)\n",
    "    test = pd.read_csv(base_path + test_file)\n",
    "    \n",
    "    return train, test\n",
    "    \n",
    "    \n",
    "def labels_encoding(train, test):\n",
    "    '''Find the number of classes and encodes the classes to integers'''\n",
    "    \n",
    "    train_data = train.copy()\n",
    "    test_data = test.copy()\n",
    "    train_data[\"label\"] = LabelEncoder().fit_transform(train_data[\"pathology\"]).astype(np.int32)\n",
    "    test_data[\"label\"] = LabelEncoder().fit_transform(test_data[\"pathology\"]).astype(np.int32)\n",
    "    \n",
    "    return train_data, test_data\n",
    "    \n",
    "    \n",
    "def split_data(train, test, val_size, stratify_col=\"label\"):\n",
    "    '''Divide training data into training and validation makes a copy of test data'''\n",
    "    train_data, val_data = train_test_split(train, \n",
    "                                        test_size=val_size, \n",
    "                                        stratify=train[stratify_col], \n",
    "                                        random_state=42\n",
    "                                       )\n",
    "    test_data = test.copy()\n",
    "    total = len(train_data) + len(val_data) + len(test_data)\n",
    "    train_percent =  round(((len(train_data) * 100)/ total), 2)\n",
    "    val_percent =  round(((len(val_data) * 100)/ total), 2)\n",
    "    test_percent =  round(((len(test_data) * 100)/ total), 2)\n",
    "    \n",
    "    print(\"Train set:\", len(train_data), \"cases,\", train_percent, \"%\")\n",
    "    print(\"Validation set:\", len(val_data), \"cases,\", val_percent, \"%\")\n",
    "    print(\"Test set:\", len(test_data), \"cases,\", test_percent, \"%\")\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b0f4de-1a10-4907-8d70-a6b25d520f8d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def image_iterators(data_sets, is_resnet_vgg=False, preprocessing_techniques=None):\n",
    "#     '''\n",
    "#         Generate a data generator for each dataset \n",
    "#     '''\n",
    "#     if is_resnet_vgg:\n",
    "#         size = 224\n",
    "#     else:\n",
    "#         size = 256\n",
    "\n",
    "#     # function contain only the image and other arguments are frozen \n",
    "#     preprocessing_function = partial(image_preprocessing, \n",
    "#                                    preprocessing_techniques=preprocessing_techniques,\n",
    "#                                    is_resnet_vgg=is_resnet_vgg,\n",
    "#                                    custom_cnn_size=size, \n",
    "#                                    resnet_vgg_size=size\n",
    "#                                     )\n",
    "    \n",
    "#     # function for setup generators\n",
    "#     def dataset_builder(dataset, shuffle=False):\n",
    "#         '''\n",
    "#         Generate dataset from preprocessed images\n",
    "#         '''\n",
    "#         # gets data\n",
    "#         paths = dataset[\"image_path\"].values\n",
    "#         labels = dataset[\"label\"].values.astype(\"float32\")\n",
    "\n",
    "#         # create dataset tensorflow Dataset\n",
    "#         new_dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "#         # shuffles data\n",
    "#         if shuffle:\n",
    "#             new_dataset = new_dataset.shuffle(buffer_size=len(dataset), seed=42)\n",
    "        \n",
    "#         # loads each image for preprocessing\n",
    "#         def image_handling(path, label, preprocessing_function, size):\n",
    "#             # loads original image\n",
    "#             image = tf.io.read_file(path)\n",
    "#             # make sure image is in gray scale\n",
    "#             image = tf.image.decode_png(image, channels=1)\n",
    "#             # normalize image for model\n",
    "#             image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "#             # preprocess image and modify current numpy arrays to work with tf\n",
    "#             image = tf.numpy_function(preprocessing_function, [image], tf.float32)\n",
    "\n",
    "#             # Restore shape \n",
    "#             image.set_shape([size, size, 1])\n",
    "\n",
    "#             return image, label\n",
    "\n",
    "#         # binds variables\n",
    "#         image_handling_func = partial(image_handling,\n",
    "#                                 preprocessing_function=preprocessing_function,\n",
    "#                                 size=size)\n",
    "\n",
    "#         # map the image handling in the iterator\n",
    "#         new_dataset = new_dataset.map(image_handling_func, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "#         # batches the dataset \n",
    "#         new_dataset = new_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         return new_dataset       \n",
    "\n",
    "#     # setup generators\n",
    "#     train_data, val_data, test_data = data_sets\n",
    "    \n",
    "#     train_dset = dataset_builder(train_data, shuffle=True)\n",
    "#     val_dset = dataset_builder(val_data, shuffle=False)\n",
    "#     test_dset = dataset_builder(test_data, shuffle=False)\n",
    "    \n",
    "#     return train_dset, val_dset, test_dset\n",
    "\n",
    "# def ablation(options):    \n",
    "#     '''\n",
    "#     Creates a dictionary with the group of techniques selected by using ablation\n",
    "#     '''\n",
    "#     # by using ablation, create the combinations of techniques \n",
    "#     techniques_groups = {}\n",
    "#     techniques_groups[\"Baseline Basic Preporcessing\"] = {option:False for option in options} # no techniques\n",
    "#     techniques_groups[\"All Preporcessing Techniques\"] = {option:True for option in options} # all tecniuqes\n",
    "\n",
    "#     # removes one techniques at a time \n",
    "#     for option in options:\n",
    "#         # creates the name of each technique\n",
    "#         group_name = option.split(\"_\")[1:]\n",
    "#         group_name =  \"No \" + \" \".join(group_name).capitalize()\n",
    "#         tech_group = {}\n",
    "#         # then uses techniques for applying a boolean\n",
    "#         for technique in options: \n",
    "#             if technique != option:\n",
    "#                 tech_group[technique] = True\n",
    "#             else:\n",
    "#                 tech_group[technique] = False\n",
    "                \n",
    "#         techniques_groups[group_name] = tech_group\n",
    "\n",
    "#     return techniques_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f02bbe-2a6e-47af-a6a2-03a2003199ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # import tensorflow as tf\n",
    "# # from functools import partial\n",
    "\n",
    "# def image_iterators(data_sets, is_resnet_vgg=False, preprocessing_techniques=None):\n",
    "#     '''\n",
    "#         Generate a data generator for each dataset \n",
    "#     '''\n",
    "#     size = 224 if is_resnet_vgg else 256\n",
    "\n",
    "#     # external preprocessing (opencv, pywt etc.)\n",
    "#     def external_preprocessing(image_np, preprocessing_techniques, is_resnet_vgg, custom_cnn_size, resnet_vgg_size):\n",
    "#         # image_np is a numpy array at this point\n",
    "#         return image_preprocessing(\n",
    "#             image_np, \n",
    "#             preprocessing_techniques=preprocessing_techniques,\n",
    "#             is_resnet_vgg=is_resnet_vgg,\n",
    "#             custom_cnn_size=custom_cnn_size,\n",
    "#             resnet_vgg_size=resnet_vgg_size\n",
    "#         )\n",
    "\n",
    "#     # function for setup generators\n",
    "#     def dataset_builder(dataset, shuffle=False):\n",
    "#         paths = dataset[\"image_path\"].values\n",
    "#         labels = dataset[\"label\"].values.astype(\"float32\")\n",
    "\n",
    "#         new_dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "#         if shuffle:\n",
    "#             new_dataset = new_dataset.shuffle(buffer_size=len(dataset), seed=42)\n",
    "\n",
    "#         def image_handling(path, label):\n",
    "#             # load raw image in TF\n",
    "#             image = tf.io.read_file(path)\n",
    "#             image = tf.image.decode_png(image, channels=1)\n",
    "#             image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "#             # apply non-TF preprocessing with numpy_function\n",
    "#             image = tf.numpy_function(\n",
    "#                 func=external_preprocessing,\n",
    "#                 inp=[image, preprocessing_techniques, is_resnet_vgg, size, size],\n",
    "#                 Tout=tf.float32\n",
    "#             )\n",
    "\n",
    "#             # restore shape (important!)\n",
    "#             image.set_shape([size, size, 1])\n",
    "\n",
    "#             return image, label\n",
    "\n",
    "#         new_dataset = new_dataset.map(image_handling, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "#         new_dataset = new_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "#         return new_dataset\n",
    "\n",
    "#     train_data, val_data, test_data = data_sets\n",
    "#     train_dset = dataset_builder(train_data, shuffle=True)\n",
    "#     val_dset = dataset_builder(val_data, shuffle=False)\n",
    "#     test_dset = dataset_builder(test_data, shuffle=False)\n",
    "\n",
    "#     return train_dset, val_dset, test_dset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfab936-68e8-4602-bb40-6c7315afd215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a088615d-2cfa-4b1a-853e-997d82002e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Image Iterators\n",
    "    \n",
    "def dataset_builder(dataset, is_resnet_vgg=False, preprocessing_techniques=None, shuffle=False):\n",
    "\n",
    "    # sets size\n",
    "    if is_resnet_vgg:\n",
    "        size = 224\n",
    "    else:\n",
    "        size = 256\n",
    "\n",
    "    # function contain only the image and other arguments are frozen \n",
    "    preprocessing_function = partial(image_preprocessing, \n",
    "                                   preprocessing_techniques=preprocessing_techniques,\n",
    "                                   is_resnet_vgg=is_resnet_vgg,\n",
    "                                   custom_cnn_size=size, \n",
    "                                   resnet_vgg_size=size\n",
    "                                    )\n",
    "        \n",
    "    # gets paths and labels from dataset\n",
    "    paths = dataset[\"image_path\"].values\n",
    "    labels = dataset[\"label\"].values\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "\n",
    "    # shuffles data\n",
    "    if shuffle:\n",
    "        new_dataset = new_dataset.shuffle(buffer_size=len(dataset), seed=42)\n",
    "\n",
    "    \n",
    "\n",
    "    # loads each \n",
    "    # def image_handling(path, label):\n",
    "    def preprocess(path, label):\n",
    "        # loads original image\n",
    "        image = tf.io.read_file(path)\n",
    "        # make sure image is in gray scale\n",
    "        image = tf.image.decode_png(image, channels=1)\n",
    "        # normalize image for model\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "\n",
    "        # preprocess images\n",
    "        image = tf.numpy_function(preprocessing_function, [image], tf.float32)\n",
    "        image.set_shape((256, 256, 1))\n",
    "        return image, label\n",
    "\n",
    "        \n",
    "        # # preprocess images\n",
    "        # image = tf.numpy_function(preprocessing_function, [image], tf.float32)\n",
    "        # # image = image.set_shape([size, size, 1])\n",
    "        # tf.print(image.shape, \"Shape 2\")\n",
    "        # tf.print(image)\n",
    "        # return image, label\n",
    "\n",
    "    preprocessed_dset = new_dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    preprocessed_dset = preprocessed_dset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "    return preprocessed_dset\n",
    "\n",
    "def image_iterators(datasets, is_resnet_vgg=False, preprocessing_techniques=None):\n",
    "    # setup generators\n",
    "    train_data, val_data, test_data = datasets\n",
    "    \n",
    "    train_dset = dataset_builder(train_data, is_resnet_vgg=is_resnet_vgg, preprocessing_techniques=preprocessing_techniques, shuffle=False)\n",
    "    val_dset = dataset_builder(val_data, is_resnet_vgg=is_resnet_vgg, preprocessing_techniques=preprocessing_techniques, shuffle=False)\n",
    "    test_dset = dataset_builder(test_data, is_resnet_vgg=is_resnet_vgg, preprocessing_techniques=preprocessing_techniques, shuffle=False)\n",
    "    \n",
    "    return train_dset, val_dset, test_dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87db1ca3-ca96-4f24-a096-c7ccc690c06d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def ablation(options):    \n",
    "    '''\n",
    "    Creates a dictionary with the group of techniques selected by using ablation\n",
    "    '''\n",
    "    # by using ablation, create the combinations of techniques \n",
    "    techniques_groups = {}\n",
    "    techniques_groups[\"Baseline Basic Preporcessing\"] = {option:False for option in options} # no techniques\n",
    "    techniques_groups[\"All Preporcessing Techniques\"] = {option:True for option in options} # all tecniuqes\n",
    "\n",
    "    # removes one techniques at a time \n",
    "    for option in options:\n",
    "        # creates the name of each technique\n",
    "        group_name = option.split(\"_\")[1:]\n",
    "        group_name =  \"No \" + \" \".join(group_name).capitalize()\n",
    "        tech_group = {}\n",
    "        # then uses techniques for applying a boolean\n",
    "        for technique in options: \n",
    "            if technique != option:\n",
    "                tech_group[technique] = True\n",
    "            else:\n",
    "                tech_group[technique] = False\n",
    "                \n",
    "        techniques_groups[group_name] = tech_group\n",
    "\n",
    "    return techniques_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "712b6e64-f8f1-474e-b0f2-49be704f6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fd8aa9-b9e9-4f80-aa51-67732f98fa09",
   "metadata": {},
   "source": [
    "## Data Preparation and Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48dafdbc-1a59-46a5-af1f-f02c1b5faaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads data\n",
    "train_df, test_df = data_loading(\"train_full.csv\", \"test_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752aa8a8-48cf-4e0d-9a48-08296445fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe and transform(encodes) pathology labels\n",
    "train_df, test_df = labels_encoding(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10be4e22-de46-48c4-bc98-1e31d838123f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['image_id', 'image_type', 'image_path', 'series_uid', 'subject_id',\n",
       "       'study_uid', 'breast_density', 'breast_side', 'image_view',\n",
       "       'abnormality_type', 'pathology', 'split', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7183faf0-daa6-4c69-aa1f-2666ad12ee5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 1889 cases, 70.35 %\n",
      "Validation set: 234 cases, 8.72 %\n",
      "Test set: 562 cases, 20.93 %\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "train_data, val_data, test_data = split_data(train_df, test_df, 0.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fdac7c-0c5b-459d-a5d2-7807efac11b7",
   "metadata": {},
   "source": [
    "## Create Preporcessing Techniques Groups: Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72f80e9d-08da-4a56-b96d-5180b5ae7378",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create groups using ablation\n",
    "options = ['apply_background_removal',\n",
    "           'apply_crop',\n",
    "           'apply_noise_reduction',\n",
    "           'apply_contrast_enhancement',\n",
    "           'apply_edge_enhancement',\n",
    "           'apply_lbp_texturizer']\n",
    "\n",
    "y_true = test_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fc38416-7987-4c9b-a08e-d0fc2eea4741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create group of techniques to try\n",
    "techniques_groups = ablation(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7569ce-07d1-4e4b-9396-302899e678c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Baseline Basic Preporcessing', 'All Preporcessing Techniques', 'No Background removal', 'No Crop', 'No Noise reduction', 'No Contrast enhancement', 'No Edge enhancement', 'No Lbp texturizer'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "techniques_groups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2191e7-617c-4680-912f-1e56bf071fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_techniques = techniques_groups['All Preporcessing Techniques']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17149a58-8d2b-4798-afe5-a24383a3ce6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data10 = train_data[:10]\n",
    "val_data10 = val_data[:5]\n",
    "test_data10 = test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "c80be5af-00ec-4ded-90da-35d9e4ef3413",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_techniques = {'apply_background_removal': False,\n",
    "                      'apply_crop': True,\n",
    "                      'apply_noise_reduction': False,\n",
    "                      'apply_contrast_enhancement': False,\n",
    "                      'apply_edge_enhancement': False,\n",
    "                      'apply_lbp_texturizer': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb216c67-f180-49e0-95a0-627a68db1a52",
   "metadata": {},
   "source": [
    "# Iterator try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fdff606-47e5-4790-8e01-522cc1642ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_builder = dataset_builder(train_data10, \n",
    "                            is_resnet_vgg=False, \n",
    "                            preprocessing_techniques=baseline_techniques, \n",
    "                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d42b9931-be5a-4612-8b71-13a3d77d1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.727356\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.418091\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.867249\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.855606\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.824188\n",
      "(<tf.Tensor: shape=(10, 256, 256, 1), dtype=float32, numpy=\n",
      "array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 0, 1, 1, 0, 0, 1, 1, 0], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for element in d_builder:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c96860-bc4b-48ed-8c2c-d8023261f03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "002ab3ba-16dc-4d38-b66d-a2921500e118",
   "metadata": {},
   "source": [
    "## Iterate trough images  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb1b05a1-e6f7-4fe2-bce1-d12f8f397894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_generator, val_generator, test_generator = image_iterators((train_data10, val_data10, test_data10), \n",
    "                                                        is_resnet_vgg=False,\n",
    "                                                        preprocessing_techniques=baseline_techniques\n",
    "                                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "96535d27-75cf-463e-92de-5cf3bf127608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1bbe94e2-ff07-44dc-8870-20bd52eec069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.727356\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.418091\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 255\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.867249\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.855606\n",
      "DEBUG 2: shape = [256 256 1] dtype = dtype('float32') min = 0 max = 254.824188\n",
      "(<tf.Tensor: shape=(10, 256, 256, 1), dtype=float32, numpy=\n",
      "array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32)>, <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 0, 1, 1, 0, 0, 1, 1, 1], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_generator.take(1):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8def84-2fbe-4e8f-8fb7-60bccd4600cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0cd28a-0f76-4d27-80c9-5f88d9d02cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create file and Save Images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DQNvenv)",
   "language": "python",
   "name": "dqnvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
